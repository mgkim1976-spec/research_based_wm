import requests
from bs4 import BeautifulSoup
from datetime import datetime
from app.models.resources import ResearchReport
from typing import List
import re
import json
import os

class MiraeResearchCrawler:
    BASE_URL = "https://securities.miraeasset.com/bbs/board/message/list.do?categoryId=1521"
    VIEW_BYPASS_URL = "https://securities.miraeasset.com/bbs/board/message/view.do?messageId={}&messageNumber={}&categoryId=1521&searchStartYear=2024&searchStartMonth=01&searchStartDay=01&searchEndYear=2026&searchEndMonth=12&searchEndDay=31"
    
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        self.db_path = "data/research_db.json"
        os.makedirs("data", exist_ok=True)
        
    def fetch_recent_reports(self, limit: int = 10) -> List[ResearchReport]:
        """Fetches the most recent research reports from the board."""
        response = requests.get(self.BASE_URL, headers=self.headers)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.find_all('table')
        
        if len(tables) < 2:
            raise ValueError("Could not find the report list table on the page.")
            
        list_table = tables[1]
        rows = list_table.find('tbody').find_all('tr')
        
        reports = []
        for row in rows[:limit]:
            cols = row.find_all('td')
            if len(cols) < 4:
                continue
                
            date_str = cols[0].get_text(strip=True)
            try:
                report_date = datetime.strptime(date_str, "%Y-%m-%d")
            except ValueError:
                report_date = datetime.now()
                
            title_td = cols[1]
            title_a = title_td.find('a')
            if not title_a:
                continue
                
            # Extracts '2338320','2622' from "javascript:view('2338320','2622')"
            href = title_a.get('href', '')
            match = re.search(r"view\('(\d+)','(\d+)'\)", href)
            if match:
                message_id = match.group(1)
                message_number = match.group(2)
                source_url = self.VIEW_BYPASS_URL.format(message_id, message_number)
                report_id = f"mirae_{message_id}"
            else:
                source_url = ""
                report_id = f"mirae_unknown_{date_str}"
                
            full_title = title_a.get_text(separator=" ", strip=True)
            
            author_str = cols[3].get_text(strip=True)
            
            # Extract PDF attachments if available
            attach_td = cols[2]
            attach_a = attach_td.find('a')
            attachment_urls = []
            if attach_a and 'downConfirm(' in attach_a.get('href', ''):
                pdf_href = attach_a.get('href', '')
                pdf_match = re.search(r"downConfirm\('(https?://[^']+)", pdf_href)
                if pdf_match:
                    attachment_urls.append(pdf_match.group(1))
            
            report = ResearchReport(
                report_id=report_id,
                title=full_title,
                date=report_date,
                author=author_str,
                report_type="Daily Market / Theme",
                source_url=source_url,
                attachment_urls=attachment_urls
            )
            reports.append(report)
            
        return reports

    def fetch_report_contents(self, report: ResearchReport):
        """Fetches the full text content of a specific report."""
        if not report.source_url:
            return ""
            
        try:
            response = requests.get(report.source_url, headers=self.headers)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            content_div = soup.find(id="messageContentsDiv")
            if content_div:
                # Get text with separators for better readability
                text = content_div.get_text(separator="\n", strip=True)
                report.normalized_text = text
                
                # Double check for PDF links if not found in list page
                if not report.attachment_urls:
                    all_scripts = soup.find_all('script')
                    for script in all_scripts:
                        if script.string:
                            pdf_match = re.search(r"Popup\.open\('(https?://[^']+)'", script.string)
                            if pdf_match:
                                report.attachment_urls.append(pdf_match.group(1))
                                
                return text
        except Exception as e:
            print(f"Error fetching report contents: {e}")
            
        return ""

    def save_reports(self, reports: List[ResearchReport]):
        """Persists reports to a JSON file, keeping history."""
        existing = self.load_all_reports()
        existing_ids = {r.report_id for r in existing}
        
        new_count = 0
        for r in reports:
            if r.report_id not in existing_ids:
                existing.append(r)
                new_count += 1
                
        # Sort by date descending
        existing.sort(key=lambda x: x.date, reverse=True)
        
        with open(self.db_path, "w", encoding="utf-8") as f:
            json.dump([r.dict() for r in existing], f, ensure_ascii=False, indent=2, default=str)
        return new_count

    def load_all_reports(self) -> List[ResearchReport]:
        """Loads all stored reports from the JSON file."""
        if not os.path.exists(self.db_path):
            return []
        try:
            with open(self.db_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return [ResearchReport(**item) for item in data]
        except Exception:
            return []

if __name__ == "__main__":
    crawler = MiraeResearchCrawler()
    reports = crawler.fetch_recent_reports(2)
    for r in reports:
        print(f"[{r.date.strftime('%Y-%m-%d')}] {r.title} by {r.author}")
        if r.attachment_urls:
            print(f" PDF: {r.attachment_urls[0]}")
